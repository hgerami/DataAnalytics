{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to construct decision trees using Entropy and Information Gain to select the splitting attribute and split point for the selected attribute:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utililty:contains utility functions that help you build a decision tree\n",
    "\n",
    "### Utililty class: implement the functions to compute entropy, information gain, perform splitting, and find the best variable (attribute) and split-point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np  # http://www.numpy.org\n",
    "import ast\n",
    "from datetime import datetime\n",
    "from math import log, floor, ceil\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Utility(object):\n",
    "\n",
    "    # This method computes entropy for information gain\n",
    "    def entropy(self, class_y):\n",
    "        # Input:\n",
    "        #   class_y         : list of class labels (0's and 1's)\n",
    "\n",
    "        # TODO: Compute the entropy for a list of classes\n",
    "        #\n",
    "        # Example:\n",
    "        #    entropy([0,0,0,1,1,1,1,1,1]) = 0.918 (rounded to three decimal places)\n",
    "\n",
    "        entropy = 0\n",
    "        \n",
    "        #############################################\n",
    "\n",
    "        if len(class_y)==0:\n",
    "            entropy = 0\n",
    "        else:\n",
    "            PA = class_y.count(0)/len(class_y)\n",
    "            PB = class_y.count(1)/len(class_y)\n",
    "\n",
    "            # to avoid nan when P is 0, if P is zero--> entropy = 0\n",
    "            if PA >0:\n",
    "                if PB>0:\n",
    "                    entropy = -PA*np.log2(PA)-PB*np.log2(PB)\n",
    "                else:\n",
    "                    entropy = -(PA*np.log2(PA))\n",
    "\n",
    "            else:\n",
    "                entropy = -(PB*np.log2(PB))\n",
    "\n",
    "\n",
    "        #############################################\n",
    "        return entropy\n",
    "\n",
    "\n",
    "    def partition_classes(self, X, y, split_attribute, split_val):\n",
    "        # Inputs:\n",
    "        #   X               : data containing all attributes\n",
    "        #   y               : labels\n",
    "        #   split_attribute : column index of the attribute to split on\n",
    "        #   split_val       : a numerical value to divide the split_attribute\n",
    "\n",
    "\n",
    "\n",
    "        # TODO: Partition the data(X) and labels(y) based on the split value - BINARY SPLIT.\n",
    "        #\n",
    "        # Split_val should be a numerical value\n",
    "        # For example, your split_val could be the mean of the values of split_attribute\n",
    "        #\n",
    "        # You can perform the partition in the following way\n",
    "        # Numeric Split Attribute:\n",
    "        #   Split the data X into two lists(X_left and X_right) where the first list has all\n",
    "        #   the rows where the split attribute is less than or equal to the split value, and the\n",
    "        #   second list has all the rows where the split attribute is greater than the split\n",
    "        #   value. Also create two lists(y_left and y_right) with the corresponding y labels.\n",
    "\n",
    "\n",
    "\n",
    "        '''\n",
    "        Example:\n",
    "\n",
    "\n",
    "\n",
    "        X = [[3, 10],                 y = [1,\n",
    "             [1, 22],                      1,\n",
    "             [2, 28],                      0,\n",
    "             [5, 32],                      0,\n",
    "             [4, 32]]                      1]\n",
    "\n",
    "\n",
    "\n",
    "        Here, columns 0 and 1 represent numeric attributes.\n",
    "\n",
    "\n",
    "\n",
    "        Consider the case where we call the function with split_attribute = 0 and split_val = 3 (mean of column 0)\n",
    "        Then we divide X into two lists - X_left, where column 0 is <= 3  and X_right, where column 0 is > 3.\n",
    "\n",
    "\n",
    "\n",
    "        X_left = [[3, 10],                 y_left = [1,\n",
    "                  [1, 22],                           1,\n",
    "                  [2, 28]]                           0]\n",
    "\n",
    "\n",
    "\n",
    "        X_right = [[5, 32],                y_right = [0,\n",
    "                   [4, 32]]                           1]\n",
    "\n",
    "\n",
    "\n",
    "        '''\n",
    "\n",
    "        X_left = []\n",
    "        X_right = []\n",
    "\n",
    "        y_left = []\n",
    "        y_right = []\n",
    "       \n",
    "        #############################################\n",
    "        left_index = [L[split_attribute]<=split_val for L in X]\n",
    "        right_index = [L[split_attribute]>split_val for L in X]\n",
    "\n",
    "        X_left = [X[i] for i in range(len(X)) if left_index[i]]\n",
    "        X_right =[X[i] for i in range(len(X)) if right_index[i]]\n",
    "\n",
    "        y_left = [y[i] for i in range(len(X)) if left_index[i]]\n",
    "        y_right = [y[i] for i in range(len(X)) if right_index[i]]\n",
    "        #############################################\n",
    "        return (X_left, X_right, y_left, y_right)\n",
    "\n",
    "\n",
    "    def information_gain(self, previous_y, current_y):\n",
    "        # Inputs:\n",
    "        #   previous_y: the distribution of original labels (0's and 1's)\n",
    "        #   current_y:  the distribution of labels after splitting based on a particular\n",
    "        #               split attribute and split value\n",
    "\n",
    "        # TODO: Compute and return the information gain from partitioning the previous_y labels\n",
    "        # into the current_y labels.\n",
    "        # You will need to use the entropy function above to compute information gain\n",
    "        # Reference: http://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15381-s06/www/DTs.pdf\n",
    "\n",
    "        \"\"\"\n",
    "        Example:\n",
    "\n",
    "        previous_y = [0,0,0,1,1,1]\n",
    "        current_y = [[0,0], [1,1,1,0]]\n",
    "\n",
    "        info_gain = 0.45915\n",
    "        \"\"\"\n",
    "\n",
    "        info_gain = 0\n",
    "        ### Implement your code here\n",
    "        #############################################\n",
    "        PL = len(current_y[0])/len(previous_y)\n",
    "        PR = len(current_y[1])/len(previous_y)\n",
    "\n",
    "        info_gain = self.entropy(previous_y) - (self.entropy(current_y[0])*PL+self.entropy(current_y[1])*PR)\n",
    "        #############################################\n",
    "        return info_gain\n",
    "\n",
    "\n",
    "    def best_split(self, X, y):\n",
    "        # Inputs:\n",
    "        #   X       : Data containing all attributes\n",
    "        #   y       : labels\n",
    "        #   TODO    : For each node find the best split criteria and return the split attribute,\n",
    "        #             spliting value along with  X_left, X_right, y_left, y_right (using partition_classes)\n",
    "        #             in the dictionary format {'split_attribute':split_attribute, 'split_val':split_val,\n",
    "        #             'X_left':X_left, 'X_right':X_right, 'y_left':y_left, 'y_right':y_right, 'info_gain':info_gain}\n",
    "        '''\n",
    "\n",
    "        Example:\n",
    "\n",
    "        X = [[3, 10],                 y = [1,\n",
    "             [1, 22],                      1,\n",
    "             [2, 28],                      0,\n",
    "             [5, 32],                      0,\n",
    "             [4, 32]]                      1]\n",
    "\n",
    "        Starting entropy: 0.971\n",
    "\n",
    "        Calculate information gain at splits: (In this example, we are testing all values in an\n",
    "        attribute as a potential split value, but you can experiment with different values in your implementation)\n",
    "\n",
    "        feature 0:  -->    split_val = 1  -->  info_gain = 0.17\n",
    "                           split_val = 2  -->  info_gain = 0.01997\n",
    "                           split_val = 3  -->  info_gain = 0.01997\n",
    "                           split_val = 4  -->  info_gain = 0.32\n",
    "                           split_val = 5  -->  info_gain = 0\n",
    "\n",
    "                           best info_gain = 0.32, best split_val = 4\n",
    "\n",
    "\n",
    "        feature 1:  -->    split_val = 10  -->  info_gain = 0.17\n",
    "                           split_val = 22  -->  info_gain = 0.41997\n",
    "                           split_val = 28  -->  info_gain = 0.01997\n",
    "                           split_val = 32  -->  info_gain = 0\n",
    "\n",
    "                           best info_gain = 0.4199, best split_val = 22\n",
    "\n",
    "\n",
    "       best_split_feature: 1\n",
    "       best_split_val: 22\n",
    "\n",
    "       'X_left': [[3, 10], [1, 22]]\n",
    "       'X_right': [[2, 28],[5, 32], [4, 32]]\n",
    "\n",
    "       'y_left': [1, 1]\n",
    "       'y_right': [0, 0, 1]\n",
    "        '''\n",
    "\n",
    "        #split_attribute = 0\n",
    "        #split_val = 0\n",
    "        X_left, X_right, y_left, y_right = [], [], [], []\n",
    "        ### Implement your code here\n",
    "        #############################################\n",
    "        split_attribute, split_val, bestIG, bestGroups  = 0, 0, 0, None\n",
    "        for index in range(len(X[0])):\n",
    "            previous_y = y\n",
    "\n",
    "            val_lst = np.unique([item[index] for item in X])\n",
    "            for val in val_lst:\n",
    "                    groups = self.partition_classes(X, y, index, val)\n",
    "                    current_y = groups[2:4]\n",
    "#                     print(previous_y,current_y)\n",
    "                    infoGain = self.information_gain(previous_y, current_y)\n",
    "#                     print('feature%d < %.3f infoGain=%.3f' % ((index), val, infoGain))\n",
    "\n",
    "                    if infoGain > bestIG:\n",
    "                        split_attribute, split_val, bestIG, bestGroups = index, val, infoGain, groups\n",
    "\n",
    "        return {'split_attribute':split_attribute, 'split_val':split_val, 'X_left':bestGroups[0],\n",
    "         'X_right':bestGroups[1], 'y_left':bestGroups[2], 'y_right':bestGroups[3],\n",
    "         'info_gain':bestIG}\n",
    "\n",
    "        #############################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[3, 10],                 \n",
    "    [1, 22],                  \n",
    "    [2, 28],                 \n",
    "    [5, 32],              \n",
    "    [4, 32]] \n",
    "\n",
    "y = [1,\n",
    "    1,\n",
    "    0,\n",
    "    0,\n",
    "    1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=  [[10, 115, 0, 0, 0, 35.3, 0.134, 29], [10, 115, 0, 0, 0, 35.3, 0.134, 29], [8, 112, 72, 0, 0, 23.6, 0.84, 58], [8, 107, 80, 0, 0, 24.6, 0.856, 34], [8, 112, 72, 0, 0, 23.6, 0.84, 58], [10, 115, 0, 0, 0, 0.0, 0.261, 30], [10, 115, 98, 0, 0, 24.0, 1.022, 34], [10, 115, 0, 0, 0, 0.0, 0.261, 30]]\n",
    "y=  [0, 0, 0, 0, 0, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'split_attribute': 5,\n",
       " 'split_val': 0.0,\n",
       " 'X_left': [[10, 115, 0, 0, 0, 0.0, 0.261, 30],\n",
       "  [10, 115, 0, 0, 0, 0.0, 0.261, 30]],\n",
       " 'X_right': [[10, 115, 0, 0, 0, 35.3, 0.134, 29],\n",
       "  [10, 115, 0, 0, 0, 35.3, 0.134, 29],\n",
       "  [8, 112, 72, 0, 0, 23.6, 0.84, 58],\n",
       "  [8, 107, 80, 0, 0, 24.6, 0.856, 34],\n",
       "  [8, 112, 72, 0, 0, 23.6, 0.84, 58],\n",
       "  [10, 115, 98, 0, 0, 24.0, 1.022, 34]],\n",
       " 'y_left': [1, 1],\n",
       " 'y_right': [0, 0, 0, 0, 0, 0],\n",
       " 'info_gain': 0.8112781244591328}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Utility().best_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTree: a decision tree class that you will use to build your random forest\n",
    "\n",
    "### Implement the learn() method to build your decision tree using the utility functions above. \n",
    "\n",
    "### implement the classify() method to predict the label of a test record using your decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    def __init__(self, max_depth):\n",
    "        # Initializing the tree as an empty dictionary or list, as preferred\n",
    "        self.tree = {}\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    \t\n",
    "    def learn(self, X, y, par_node = {}, depth=0):\n",
    "        # TODO: Train the decision tree (self.tree) using the the sample X and labels y\n",
    "        # You will have to make use of the functions in Utility class to train the tree\n",
    "\n",
    "        # par_node is a parameter that is useful to pass additional information to call\n",
    "        # the learn method recursively. Its not mandatory to use this parameter\n",
    "\n",
    "        # Use the function best_split in Utility class to get the best split and\n",
    "        # data corresponding to left and right child nodes\n",
    "\n",
    "        # One possible way of implementing the tree:\n",
    "        #    Each node in self.tree could be in the form of a dictionary:\n",
    "        #       https://docs.python.org/2/library/stdtypes.html#mapping-types-dict\n",
    "        #    For example, a non-leaf node with two children can have a 'left' key and  a\n",
    "        #    'right' key. You can add more keys which might help in classification\n",
    "        #    (eg. split attribute and split value)\n",
    "        ### Implement your code here\n",
    "        #############################################\n",
    "\n",
    "\n",
    "        # stopping the split when entropy is low (almost pure leaf)\n",
    "        en_y = Utility().entropy(y)\n",
    "        if en_y < .005:\n",
    "            label = max(set(y), key = y.count)\n",
    "            return label\n",
    "\n",
    "        # stopping the split when all labels in y are the same\n",
    "        elif len(set(y)) == 1:\n",
    "            label = max(set(y), key = y.count)\n",
    "            return label\n",
    "\n",
    "\n",
    "        # stopping the split if we have reached maximum depth\n",
    "        elif depth >= self.max_depth:\n",
    "            label = max(set(y), key = y.count)\n",
    "            return label\n",
    "\n",
    "        else:\n",
    "            X_left = Utility().best_split(X, y)['X_left']\n",
    "            y_left = Utility().best_split(X, y)['y_left']\n",
    "            X_right = Utility().best_split(X, y)['X_right']\n",
    "            y_right = Utility().best_split(X, y)['y_right']\n",
    "            split_val = Utility().best_split(X, y)['split_val']\n",
    "            split_attribute = Utility().best_split(X, y)['split_attribute']\n",
    "#             info_gain = Utility().best_split(X, y)['info_gain']\n",
    "\n",
    "            par_node = {}\n",
    "            par_node['attr'] = split_attribute\n",
    "            par_node['value'] = split_val\n",
    "            par_node['left'] = self.learn(X_left, y_left, {} , depth+1)\n",
    "            par_node['right'] = self.learn(X_right, y_right, {} , depth+1)\n",
    "\n",
    "            self.tree = par_node\n",
    "            return par_node\n",
    "        #############################################\n",
    "\n",
    "\n",
    "    def classify(self, record):\n",
    "        # TODO: classify the record using self.tree and return the predicted label\n",
    "        ### Implement your code here\n",
    "        #############################################\n",
    "        tree = self.tree\n",
    "        #print(tree)\n",
    "        while isinstance(tree, dict):\n",
    "            if record[tree['attr']] <= tree['value']:\n",
    "                tree = tree['left']\n",
    "            else:\n",
    "                tree = tree['right']\n",
    "        return tree\n",
    "        #############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTree(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.tree = clf.learn(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attr': 5, 'value': 0.0, 'left': 1, 'right': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.classify(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.max_depth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attr': 5, 'value': 0.0, 'left': 1, 'right': 0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-Bag Error Estimate. In random forests, it is not necessary to perform explicit cross-validation or use a separate test set for performance evaluation. Out-of-bag (OOB) error estimate has shown to be reasonably accurate and unbiased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implement the methods _bootstrapping(), fitting(), voting() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest(object):\n",
    "    num_trees = 0\n",
    "    decision_trees = []\n",
    "\n",
    "    # the bootstrapping datasets for trees\n",
    "    # bootstraps_datasets is a list of lists, where each list in bootstraps_datasets is a bootstrapped dataset.\n",
    "    bootstraps_datasets = []\n",
    "\n",
    "    # the true class labels, corresponding to records in the bootstrapping datasets\n",
    "    # bootstraps_labels is a list of lists, where the 'i'th list contains the labels corresponding to records in\n",
    "    # the 'i'th bootstrapped dataset.\n",
    "    bootstraps_labels = []\n",
    "\n",
    "    def __init__(self, num_trees):\n",
    "        # Initialization done here\n",
    "        self.num_trees = num_trees\n",
    "        self.decision_trees = [DecisionTree(max_depth=10) for i in range(num_trees)]\n",
    "        self.bootstraps_datasets = []\n",
    "        self.bootstraps_labels = []\n",
    "\n",
    "    def _bootstrapping(self, XX, n):\n",
    "        # Reference: https://en.wikipedia.org/wiki/Bootstrapping_(statistics)\n",
    "        #\n",
    "        # TODO: Create a sample dataset of size n by sampling with replacement\n",
    "        #       from the original dataset XX.\n",
    "        # Note that you would also need to record the corresponding class labels\n",
    "        # for the sampled records for training purposes.\n",
    "\n",
    "        sample = [] # sampled dataset\n",
    "        labels = []  # class labels for the sampled records\n",
    "        ### Implement your code here\n",
    "        #############################################\n",
    "        for i in range(n):\n",
    "            arr = np.random.choice(len(XX),n,replace=True)\n",
    "            sample = [XX[i][:-1] for i in arr]\n",
    "            labels = [XX[i][-1] for i in arr]\n",
    "        #############################################\n",
    "        return (sample, labels)\n",
    "\n",
    "    def bootstrapping(self, XX):\n",
    "        # Initializing the bootstap datasets for each tree\n",
    "        for i in range(self.num_trees):\n",
    "            data_sample, data_label = self._bootstrapping(XX, len(XX))\n",
    "            self.bootstraps_datasets.append(data_sample)\n",
    "            self.bootstraps_labels.append(data_label)\n",
    "\n",
    "    def fitting(self):\n",
    "        # TODO: Train `num_trees` decision trees using the bootstraps datasets\n",
    "        # and labels by calling the learn function from your DecisionTree class.\n",
    "        ### Implement your code here\n",
    "        #############################################\n",
    "        for i in range(self.num_trees):\n",
    "\n",
    "            np.random.seed(18)\n",
    "\n",
    "            self.decision_trees[i].learn(self.bootstraps_datasets[i],self.bootstraps_labels[i])\n",
    "        #############################################\n",
    "\n",
    "    def voting(self, X):\n",
    "        y = []\n",
    "        for record in X:\n",
    "            # Following steps have been performed here:\n",
    "            #   1. Find the set of trees that consider the record as an\n",
    "            #      out-of-bag sample.\n",
    "            #   2. Predict the label using each of the above found trees.\n",
    "            #   3. Use majority vote to find the final label for this recod.\n",
    "            votes = []\n",
    "\n",
    "            for i in range(len(self.bootstraps_datasets)):\n",
    "                dataset = self.bootstraps_datasets[i]\n",
    "\n",
    "                if record not in dataset:\n",
    "                    OOB_tree = self.decision_trees[i]\n",
    "                    effective_vote = OOB_tree.classify(record)\n",
    "                    votes.append(effective_vote)\n",
    "            counts = np.bincount(votes)\n",
    "\n",
    "            if len(counts) == 0:\n",
    "                # TODO: Special case\n",
    "                #  Handle the case where the record is not an out-of-bag sample\n",
    "                #  for any of the trees.\n",
    "                # NOTE - you can add few lines of codes above (but inside voting) to make this work\n",
    "                ### Implement your code here\n",
    "                #############################################\n",
    "                idx = self.bootstraps_datasets[0].index(record) # len(counts)==0 --> record exists in all bootstraps_datasets\n",
    "                y = np.append(y, self.bootstraps_labels[0][idx]) # get the label for that record and append to y\n",
    "                #############################################\n",
    "            else:\n",
    "                y = np.append(y, np.argmax(counts))\n",
    "\n",
    "        return y\n",
    "\n",
    "    def user(self):\n",
    "        \"\"\"\n",
    "        :return: string\n",
    "    \n",
    "        \"\"\"\n",
    "        ### Implement your code here\n",
    "        #############################################\n",
    "        return 'Homayoun Gerami'\n",
    "        #############################################\n",
    "\n",
    "\n",
    "# TODO: Determine the forest size according to your implementation.\n",
    "# This function will be used by the autograder to set your forest size during testing\n",
    "# VERY IMPORTANT: Minimum forest_size should be 10\n",
    "def get_forest_size():\n",
    "    forest_size = 10\n",
    "    return forest_size\n",
    "\n",
    "# TODO: Determine random seed to set for reproducibility\n",
    "# This function will be used by the autograder to set the random seed to obtain the same results you achieve locally\n",
    "def get_random_seed():\n",
    "    random_seed = 0\n",
    "    return random_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the data:\n",
    "The dataset you will use is pima-indians-diabetes.csv, a comma-separated (csv) file. The\n",
    "dataset was derived from National Institute of Diabetes and Digestive and Kidney Diseases. Each row describes one person (a data point, or data record) using 9 columns. The first 8 are attributes. The 9th is the label and it should NOT be treated as an attribute. We will run binary classification on the dataset to determine if a person has a diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    np.random.seed(get_random_seed())\n",
    "    # start time \n",
    "    start = datetime.now()\n",
    "    X = list()\n",
    "    y = list()\n",
    "    XX = list()  # Contains data features and data labels\n",
    "    numerical_cols = set([i for i in range(0, 9)])  # indices of numeric attributes (columns)\n",
    "\n",
    "    # Loading data set\n",
    "    print(\"reading the data\")\n",
    "    with open(\"pima-indians-diabetes.csv\") as f:\n",
    "        next(f, None)\n",
    "        for line in csv.reader(f, delimiter=\",\"):\n",
    "            xline = []\n",
    "            for i in range(len(line)):\n",
    "                if i in numerical_cols:\n",
    "                    xline.append(ast.literal_eval(line[i]))\n",
    "                else:\n",
    "                    xline.append(line[i])\n",
    "\n",
    "            X.append(xline[:-1])\n",
    "            y.append(xline[-1])\n",
    "            XX.append(xline[:])\n",
    "\n",
    "    # Initializing a random forest.\n",
    "    randomForest = RandomForest(get_forest_size())\n",
    "\n",
    "    # printing the name\n",
    "    print(\"__Name: \" + randomForest.user()+\"__\")\n",
    "\n",
    "    # Creating the bootstrapping datasets\n",
    "    print(\"creating the bootstrap datasets\")\n",
    "    randomForest.bootstrapping(XX)\n",
    "\n",
    "    # Building trees in the forest\n",
    "    print(\"fitting the forest\")\n",
    "    randomForest.fitting()\n",
    "\n",
    "    # Calculating an unbiased error estimation of the random forest\n",
    "    # based on out-of-bag (OOB) error estimate.\n",
    "    y_predicted = randomForest.voting(X)\n",
    "\n",
    "    # Comparing predicted and true labels\n",
    "    results = [prediction == truth for prediction, truth in zip(y_predicted, y)]\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = float(results.count(True)) / float(len(results))\n",
    "\n",
    "    print(\"accuracy: %.4f\" % accuracy)\n",
    "    print(\"OOB estimate: %.4f\" % (1 - accuracy))\n",
    "\n",
    "    # end time\n",
    "    print(\"Execution time: \" + str(datetime.now() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading the data\n",
      "__Name: Homayoun Gerami__\n",
      "creating the bootstrap datasets\n",
      "fitting the forest\n",
      "accuracy: 0.7370\n",
      "OOB estimate: 0.2630\n",
      "Execution time: 0:02:29.302255\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "1) Georgia Tech CSE 6242: Data and Visual Analytics Course\n",
    "\n",
    "2) http://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15381-s06/www/DTs.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
