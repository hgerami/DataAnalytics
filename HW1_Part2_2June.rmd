---
title: "HW1, Part2"
author: "HG"
date: "22/05/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2) 
library(magrittr) 
library(dplyr)    
```

# **Homework-1 , Part-2**

# *QUESTION 1:*
Use the  airbnb_data.csv  provided and answer the following questions on Linear Regression:

## a) Fit a multiple linear regression model using price as the response variable and all others as predictor variables (Note: remove 'id' columns). Which variables are statistically significant in determining the price?

## b) Interpret the coefficients for predictors: room type(Shared Room), bedrooms?

## c) Predict the price (nearest dollar) for a listing with the following factors: bedrooms = 1, accommodates = 2, reviews = 70, overall_satisfaction = 4, and room_type= 'Private room'.

## d) Identify outliers using Cook's distance approach. Remove points having Cook's distance > 1. Rerun the model after the removal of these points and print the summary.


```{r }
#a)

# data loading and keeping the required attributes only (removing the oned with "id" & the city name)
airbnb<-  data.frame(read.csv("airbnb_data.csv"))
airbnb <- airbnb[, 4:10]
airbnb <- airbnb[, -2]

View(head(airbnb))

airbnb_lm <- lm(formula = price ~ . , data = airbnb)
summary (airbnb_lm)
plot(airbnb_lm)


ggplot(data = airbnb, aes(x = accommodates, y = price, colour=room_type)) + geom_point(aes(size=1)) +
scale_colour_hue(l=50) + 
geom_smooth(method=lm,   # Add linear regression lines
           se=TRUE,    #  add shaded confidence region
           fullrange=TRUE) +
theme(axis.text.x = element_text(size=20), axis.text.y = element_text(size=20), 
        axis.title=element_text(size=24,face="bold"))


```
## a) as per the model reported stats, the accomodates & bedrooms are the most significant for determining the price (highest coefficents & lowest p-values)

Coefficients:
                       Estimate Std. Error t value Pr(>|t|)    
(Intercept)           -23.36172   21.88618  -1.067  0.28609    
room_typePrivate room  -0.93115   13.21827  -0.070  0.94386    
room_typeShared room  -76.66780   59.90939  -1.280  0.20099    
reviews                 0.01090    0.09982   0.109  0.91310    
overall_satisfaction  -10.48160    3.47320  -3.018  0.00262 ** 
accommodates           23.00721    5.23952   4.391 1.27e-05 ***
bedrooms               85.64533   11.45983   7.474 1.95e-13 ***



```{r , echo=FALSE}
ggplot(data = airbnb, aes(x = accommodates, y = price, colour=room_type)) + geom_point(aes(size=1)) +
scale_colour_hue(l=50) + 
geom_smooth(method=lm,   # Add linear regression lines
           se=TRUE,    #  add shaded confidence region
           fullrange=TRUE) +
theme(axis.text.x = element_text(size=20), axis.text.y = element_text(size=20), 
        axis.title=element_text(size=24,face="bold"))
```
## b) Price = -23.36172 + (-0.93115) Private_Room + (-76.66780) Shared_Room + 0.0109 reviews + (-10.48160) overall_satisfaction + 23.0072 accomodates + 85.6453 bedrooms
Interpret the coefficients for predictors: room type(Shared Room), bedrooms

coeffiecet for shared room : -76.66780 
since it is a negative number it shows that if a room is shared one the starting price would be pretty low, starting price for a shared room = -23.36172 + (-76.6678); 


coeffiecet for bedrooms : 85.6453 
assuming other parameters are not changing, an incerease of one bedroom will incerease the pricing with rate of 85.6453$ 


```{r message=TRUE}
#c) Predict the price (nearest dollar) for a listing with the following factors: bedrooms = 1, accommodates = 2, reviews = 70, overall_satisfaction = 4, and room_type= 'Private room'
New_Point <- data.frame(bedrooms = 1, accommodates = 2, reviews = 70, overall_satisfaction = 4, room_type= 'Private room')
predict.price <- predict(airbnb_lm, New_Point)
predict.price
```
## c)
Predicted Price = 66.20316$

```{r}
#d)Identify outliers using Cook's distance approach. Remove points having Cook's distance > 1. Rerun the model after the removal of these points and print the summary. 

```
# d)
Identify outliers using Cook's distance approach. Remove points having Cook's distance > 1. Rerun the model after the removal of these points and print the summary. 

from the earlier plots the points 94, 95 & 96 looks to be differnt and maybe can be considered as outliers. Using the crtiteria "Cook's distance >1" the point 94 & 95 looks to be an outlier and was removed:

```{r}
#removing the influentials/outliers:

cooksd <- cooks.distance(airbnb_lm)

influential <- as.numeric(names(cooksd)[cooksd > 1])

influential



airbnb_no_influential <- airbnb [-influential,]  

#build a new model after removing the influnetion points 

airbnb_no_IP_lm <- lm(formula = price ~ . , data = airbnb_no_influential)
summary (airbnb_no_IP_lm)
plot(airbnb_no_IP_lm)


#build a new model after removing the influnetion points & outliers
outliers <- append (influential,96) 
outliers

airbnb_no_outliers <- airbnb [-outliers,]  
airbnb_no_OP_lm <- lm(formula = price ~ . , data = airbnb_no_outliers)
summary (airbnb_no_OP_lm)
plot(airbnb_no_OP_lm)
ggplot(data = airbnb_no_outliers, aes(x = accommodates, y = price, colour=room_type)) + geom_point(aes(size=1)) +
scale_colour_hue(l=50) + 
geom_smooth(method=lm,   # Add linear regression lines
           se=TRUE,    #  add shaded confidence region
           fullrange=TRUE) +
theme(axis.text.x = element_text(size=20), axis.text.y = element_text(size=20), 
        axis.title=element_text(size=24,face="bold"))


```
in addition to the influential points, 94 & 95, I also tested removing another value 96, as I think that value is probably  an ouutlier too

```{r}
#this is for Question#2 preperation, selecting the points with overall_satisfaction != 0.0
airbnb_no_outliers_Zero_satisfaction <- filter(airbnb_no_outliers, overall_satisfaction > 0.5)

```

the updated plot suggest that price for the room types pivate and shared are kind of constant, that make sence as the majority if these type rooms has one bed room only, and other attributes has little influenece on ther prising of these room type;
on the other hand pricing fro the room type as entire home varies as per number of bedroom and number of accomdates. 


# *QUESTION 2:* 
Use the direct_marketing.csv provided and answer the following questions on Linear Regression:
Create indicator variables for the ‘History’ column. Considering the base case as None (i.e., create Low, Medium and High variables with 1 denoting the positive case and 0 the negative) and few additional variables LowSalary, MediumSalary and HighSalary based on the customer history type i.e., MediumSalary = Medium*Salary etc.

## a) Fit a multiple linear regression model using AmountSpent as the response variable and the indicator variables along with their salary variables as the predictors

## b) What is the amount spent by a customer for each historic type provided his/her salary is $10,000 based on the model constructed in part a?
Use the airbnb_data.csv  provided and answer the following questions on Linear Regression:
Perform Log transformation for the variables price and overall_satisfaction, make necessary transformations suggested in the class.

## c) Fit all four models i.e., linear-linear, linear-log, log-linear, and log-log regression models using price as the response variable and overall_satisfaction as the predictor.

## d) Which of the four models has the best R2? Do you have any comments on the choice of the dependent variable?
```{r}

#a)

# data loading 
direct_marketing <-  data.frame(read.csv("direct_marketing.csv"))
#View(head(direct_marketing))

#creating indicator variables for the History
direct_marketing<- direct_marketing %>%
  mutate(Low_History = ifelse(History=="Low",1,0)) %>%
  mutate(Medium_History = ifelse(History=="Medium",1,0)) %>%
  mutate(High_History = ifelse(History=="High",1,0)) %>%
  mutate(LowHistory_Salary = Low_History * Salary) %>%
  mutate(MediumHistory_Salary = Medium_History * Salary) %>%
  mutate(HighHistory_Salary = High_History * Salary)
#creating linear model
direct_marketing_lm <- lm(formula = AmountSpent ~ Salary + Low_History + Medium_History + High_History + LowHistory_Salary + MediumHistory_Salary + HighHistory_Salary , data = direct_marketing)


summary (direct_marketing_lm)
plot(direct_marketing_lm)

```

# b)
AmountSpent = 1.9622199 + 0.0023641 Salary + 25.4466733 Low_History + 79.2984388 Medium_History + 72.6735221 High_History + (-0.0021069) LowHistory_Salary + (-0.0021153) MediumHistory_Salary + (-0.0006408) HighHistory_Salary
What is the amount spent by a customer for each historic type provided his/her salary is $10,000 based on the model constructed in part a?

```{r }


Low_History_Salary_1000<- data.frame(Salary = 10000, Low_History = 1, Medium_History = 0, High_History = 0, LowHistory_Salary=10000 , MediumHistory_Salary=0 , HighHistory_Salary=0 )
predict.AmountSpent.Low_History_Salary_1000 <- predict(direct_marketing_lm, Low_History_Salary_1000)
predict.AmountSpent.Low_History_Salary_1000

Medium_History_Salary_1000<- data.frame(Salary = 10000, Low_History = 0, Medium_History = 1, High_History = 0, LowHistory_Salary=0 , MediumHistory_Salary=10000 , HighHistory_Salary=0 )
predict.AmountSpent.Medium_History_Salary_1000 <- predict(direct_marketing_lm, Medium_History_Salary_1000)
predict.AmountSpent.Medium_History_Salary_1000

High_History_Salary_1000<- data.frame(Salary = 10000, Low_History = 0, Medium_History = 0, High_History = 1, LowHistory_Salary=0 , MediumHistory_Salary=0 , HighHistory_Salary=10000 )
predict.AmountSpent.High_History_Salary_1000 <- predict(direct_marketing_lm, High_History_Salary_1000)
predict.AmountSpent.High_History_Salary_1000

Null_History_Salary_1000<- data.frame(Salary = 10000, Low_History = 0, Medium_History = 0, High_History = 0, LowHistory_Salary=0 , MediumHistory_Salary=0 , HighHistory_Salary=0 )
predict.AmountSpent.Null_History_Salary_1000 <- predict(direct_marketing_lm, Null_History_Salary_1000)
predict.AmountSpent.Null_History_Salary_1000

```
Apmount Spent Low History with 10000 Salary = 29.98157
Apmount Spent Medium History with 10000 Salary = 83.74909
Apmount Spent High History with 10000 Salary = 91.86874
Apmount Spent Null History with 10000 Salary = 25.60347

# log transformation


```{r }
#c & d)
# removing the points with overall_satisfaction = 0.0
#airbnb_no_outliers_Zero_satisfaction <- filter(airbnb_no_outliers, overall_satisfaction > 0.5)

# Fit all four models i.e., linear-linear, linear-log, log-linear, and log-log regression models using price as the response variable and overall_satisfaction as the predictor

airbnb_no_outliers_Zero_satisfaction <- airbnb_no_outliers_Zero_satisfaction %>%
  mutate(Ln_price = log(price)) %>% 
  mutate(Ln_overall_satisfaction = log(overall_satisfaction))

# Model A:  price = b0 + b1*overall_satisfaction  Linear-linear model

Lin_Lin <- lm(formula = price ~ overall_satisfaction , data = airbnb_no_outliers_Zero_satisfaction)
summary(Lin_Lin)


# Model B:  price = b0 + b1*log(overall_satisfaction)  Linear-Log Model

Lin_LN <- lm(formula = price ~ Ln_overall_satisfaction , data = airbnb_no_outliers_Zero_satisfaction)
summary(Lin_LN)



# Model C:  log(price) = b0 + b1*overall_satisfaction  Log-Linear Model

LN_Lin <- lm(formula = Ln_price ~ overall_satisfaction , data = airbnb_no_outliers_Zero_satisfaction)
summary(LN_Lin)



# Model D:  log(price) = b0 + b1*log(overall_satisfaction)  Log-Log Model

LN_LN <- lm(formula = Ln_price ~ Ln_overall_satisfaction , data = airbnb_no_outliers_Zero_satisfaction)
summary(LN_LN)


```
##) d 
reported R2:

Lin-Lin
R-squared:  0.00749

Lin-Ln
R-squared:  0.007386

Ln-Lin
R-squared:  0.01178

Ln-Ln
R-squared:  0.01162

The overall R2 values are very low, vindicating there might be only a very weak relation between price & overall satisfaction. considering Ln (Price) has slightly increased R2 value. the Ln-Lin model reported the highest R2. 

# **Question 3**

The attached titanic_data.csv; It has been cleaned to remove all rows which contain missing values. We will perform logistic regression on this cleaned dataset.

The dataset contains the following columns:
‘Name’ - Passenger Name - factor
‘PClass’ - Passenger Class (1st, 2nd, 3rd) - factor
‘Age’ - Passenger Age - number
‘Sex’ - Passenger Sex – female, male
‘Survived’ – 1 if passenger survived, 0 if not - number

After converting the survived variable to be a factor with two levels, 0 and 1, perform logistic regression on the dataset using ‘survived’ as the response and ‘Sex’ as the explanatory variable.

## a) Display the model summary.
## b) What does the value of the intercept coefficient represent in this model?
## c) Determine the probability of survival for females.
## d) Determine the probability of survival for males.
```{r }
# data loading 
titanic <-  data.frame(read.csv("titanic_data.csv"))

View(head(titanic))

# converting the survived variable to be a factor with two levels
titanic$Survived <- factor(titanic$Survived)

titanic<- titanic %>%
  mutate(Sex_female = ifelse(Sex=="female",1,0)) %>%
  mutate(Sex_male = ifelse(Sex=="male",1,0))

#buidling the logistic regression:
titanic_logit <- glm(Survived ~ Sex_female , data = titanic, family = "binomial")
#a)
summary(titanic_logit)

#b) intercept interpretation:
```
b) Since the intercept is negative the survival Odds for males is low, the intercept is natual log of survival odds, and not the odds, for male

```{r}
# c & d) reported surviving probabilities for male & female:


# to calculate the survial probability for male and female the following function can be useful:

logit2prob <- function(logit){
  odds <- exp(logit)
  prob <- odds / (1 + odds)
  return(prob)
}


# we then use the coefficients of the logit model to caculate the Ln of odds and then feed them into the probability calculation function to report the survival probabilities

ln_SurvivalOdds_Male_and_Female <- c(as.numeric(coef(titanic_logit)[1]), as.numeric( coef(titanic_logit)[1] + coef(titanic_logit)[2]))
print("Survival probability for males and females: ")
logit2prob(ln_SurvivalOdds_Male_and_Female)

#however the prdict function does all the above much easier
print("Survival probability for males: ")
predict(titanic_logit, data.frame(Sex_female=0)  , type="response")

print("Survival probability for females: ")
predict(titanic_logit, data.frame(Sex_female=1)  , type="response")

```

c & d)
female surviving probability: 0.753 (~ 0.8)
male surviving probabaility: 0.205 (~ 0.2)
